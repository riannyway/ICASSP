
from modelscope import snapshot_download, AutoProcessor, Qwen2AudioForConditionalGeneration
import torch
import os
import json
from typing import Dict, List, Optional
import re
import librosa
import numpy as np
from io import BytesIO

class AudioAnalyzer:
    """éŸ³é¢‘åˆ†æå™¨ç±»"""
    
    def __init__(self, model_dir: Optional[str] = None, cache_dir: str = '/autodl-tmp/models'):
        """
        åˆå§‹åŒ–éŸ³é¢‘åˆ†æå™¨
        
        Args:
            model_dir: æ¨¡å‹ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ä¸‹è½½
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
        """
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"

        # ä¸‹è½½æˆ–åŠ è½½æ¨¡å‹
        if model_dir is None:
            print("æ­£åœ¨ä¸‹è½½æ¨¡å‹...")
            model_dir = snapshot_download('Qwen/Qwen2-Audio-7B-Instruct', cache_dir=cache_dir)
        
        print("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
        self.processor = AutoProcessor.from_pretrained(
            model_dir,
            trust_remote_code=True
        )
        
        self.model = Qwen2AudioForConditionalGeneration.from_pretrained(
            model_dir,
            device_map="auto",
            trust_remote_code=True,
            dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True
        ).eval()
        
        print(f"æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {self.model.device}")
        print(f"æ¨¡å‹è·¯å¾„: {model_dir}")
    
    def analyze_full_audio(self, audio_path: str) -> Dict:
        """
        åˆ†ææ•´ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼Œè¿”å›æŒ‰è¯´è¯äººåŒºåˆ†çš„åˆ†æç»“æœ
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        if not os.path.exists(audio_path):
            return {"error": f"éŸ³é¢‘æ–‡ä»¶ {audio_path} ä¸å­˜åœ¨"}
        
        try:
            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            print("æ­£åœ¨åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
            audio_data, sample_rate = librosa.load(
                audio_path, 
                sr=self.processor.feature_extractor.sampling_rate
            )
            
            # æ£€æŸ¥éŸ³é¢‘é•¿åº¦
            duration = len(audio_data) / sample_rate
            print(f"éŸ³é¢‘æ—¶é•¿: {duration:.2f}ç§’")
            print(f"é‡‡æ ·ç‡: {sample_rate}Hz")
            
            # æ„å»ºè¯¦ç»†çš„åˆ†ææç¤ºè¯
            prompt_text = """è¯·å¯¹è¿™æ®µéŸ³é¢‘è¿›è¡Œå…¨é¢æ·±å…¥çš„åˆ†æï¼Œè¦æ±‚å¦‚ä¸‹ï¼š

ç‰¹åˆ«æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœä½ ä¸èƒ½ç†è§£æˆ‘ä¼ é€’ç»™ä½ çš„éŸ³é¢‘ï¼Œè¯·è¯´"æ— æ³•ç†è§£éŸ³é¢‘å†…å®¹"ï¼Œè€Œä¸æ˜¯ç¼–é€ å†…å®¹ã€‚

## åˆ†æä»»åŠ¡ï¼š
1. **è¯´è¯äººè¯†åˆ«ä¸åŒºåˆ†**
   - è¯†åˆ«éŸ³é¢‘ä¸­å…±æœ‰å‡ ä½è¯´è¯äºº
   - ä¸ºæ¯ä½è¯´è¯äººåˆ†é…æ ‡è¯†ï¼ˆå¦‚ï¼šè¯´è¯äºº1ã€è¯´è¯äºº2ç­‰ï¼‰
   - æè¿°æ¯ä½è¯´è¯äººçš„å£°éŸ³ç‰¹å¾ï¼ˆæ€§åˆ«ã€å¹´é¾„æ®µä¼°è®¡ã€éŸ³è‰²ç‰¹ç‚¹ï¼‰

2. **æŒ‰è¯´è¯äººåˆ†æè¯­éŸ³ç‰¹å¾**
   å¯¹æ¯ä½è¯´è¯äººï¼Œè¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š
   
   a) **è¯­é€Ÿåˆ†æ**
      - å¹³å‡è¯­é€Ÿï¼ˆå¿«/ä¸­/æ…¢ï¼‰
      - è¯­é€Ÿå˜åŒ–æƒ…å†µï¼ˆæ˜¯å¦æœ‰æ˜æ˜¾çš„åŠ é€Ÿæˆ–å‡é€Ÿï¼‰
      - åœé¡¿æ¨¡å¼å’ŒèŠ‚å¥ç‰¹ç‚¹
   
   b) **è¯­è°ƒåˆ†æ**
      - åŸºæœ¬è¯­è°ƒç±»å‹ï¼ˆå¹³è°ƒ/å‡è°ƒ/é™è°ƒä¸»å¯¼ï¼‰
      - è¯­è°ƒå˜åŒ–èŒƒå›´ï¼ˆå•è°ƒ/é€‚ä¸­/ä¸°å¯Œï¼‰
      - æƒ…æ„Ÿè¡¨è¾¾å¼ºåº¦
   
   c) **éŸ³é«˜å˜åŒ–**
      - åŸºç¡€éŸ³é«˜æ°´å¹³ï¼ˆé«˜/ä¸­/ä½ï¼‰
      - éŸ³é«˜å˜åŒ–å¹…åº¦
      - éŸ³é«˜æ³¢åŠ¨æ¨¡å¼

3. **å†…å®¹è½¬å½•**
   - æŒ‰è¯´è¯äººåŒºåˆ†è½¬å½•å†…å®¹
   - æ ‡æ³¨æ¯æ®µè¯çš„è¯´è¯äºº

4. **æƒ…ç»ªçŠ¶æ€åˆ†æ**
   - åˆ†ææ¯ä½è¯´è¯äººçš„ä¸»è¦æƒ…ç»ªçŠ¶æ€
   - æƒ…ç»ªå˜åŒ–è½¨è¿¹

5. **äº¤äº’åˆ†æ**ï¼ˆå¦‚æœ‰å¤šäººï¼‰
   - è¯´è¯äººä¹‹é—´çš„äº’åŠ¨æ¨¡å¼
   - è¯è½®è½¬æ¢ç‰¹ç‚¹

## è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
è¯·æŒ‰ä»¥ä¸‹ç»“æ„åŒ–æ ¼å¼è¾“å‡ºï¼š

ã€éŸ³é¢‘æ¦‚è§ˆã€‘
- æ€»æ—¶é•¿ä¼°è®¡ï¼š
- è¯´è¯äººæ•°é‡ï¼š
- éŸ³é¢‘è´¨é‡ï¼š

ã€è¯´è¯äººåˆ†æã€‘
[è¯´è¯äºº1]
- å£°éŸ³ç‰¹å¾ï¼š
- è¯­é€Ÿï¼š
- è¯­è°ƒï¼š
- éŸ³é«˜ï¼š
- ä¸»è¦æƒ…ç»ªï¼š
- è½¬å½•å†…å®¹ï¼š

[è¯´è¯äºº2]
ï¼ˆåŒä¸Šæ ¼å¼ï¼‰

ã€äº¤äº’ç‰¹å¾ã€‘
- è¯è½®æ¨¡å¼ï¼š
- äº’åŠ¨ç‰¹ç‚¹ï¼š

è¯·ç¡®ä¿åˆ†æè¯¦å°½ä¸”æŒ‰è¯´è¯äººæ¸…æ™°åŒºåˆ†ã€‚è¯·å¸®æˆ‘åˆ†æè¿™æ®µéŸ³é¢‘ã€‚"""

            # æ„å»ºå¯¹è¯æ ¼å¼ - å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨æœ¬åœ°éŸ³é¢‘æ•°æ®
            conversation = [
                {"role": "user", "content": [
                    {"type": "audio", "audio_data": audio_data},  
                    {"type": "text", "text": prompt_text}
                ]}
            ]

            # å‡†å¤‡éŸ³é¢‘æ•°æ®åˆ—è¡¨
            audios = []
            for message in conversation:
                if isinstance(message["content"], list):
                    for ele in message["content"]:
                        if ele["type"] == "audio":
                            audios.append(ele["audio_data"]) 
            print("æ­£åœ¨å¤„ç†è¾“å…¥æ•°æ®...")
            
            # å‡†å¤‡æ–‡æœ¬è¾“å…¥
            text_input = self.processor.apply_chat_template(
                conversation, 
                add_generation_prompt=True, 
                tokenize=False
            )
            
            inputs = self.processor(
                text=text_input,
                audio=audios, 
                return_tensors="pt",
                sampling_rate=sample_rate,
                padding=True
            )
            
            inputs = inputs.to("cuda")

            

            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=4096,
                    temperature=0.3,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.processor.tokenizer.eos_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id,
                    repetition_penalty=1.05
                )
            
            # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
            generated_ids = [
                output_ids[len(input_ids):]
                for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            response = self.processor.batch_decode(
                generated_ids, 
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]
            
            print(f"ç”Ÿæˆçš„å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            
            # è§£æå“åº”ä¸ºç»“æ„åŒ–æ•°æ®
            parsed_result = self._parse_response(response)
            parsed_result["raw_response"] = response
            parsed_result["audio_info"] = {
                "duration": duration,
                "sample_rate": sample_rate,
                "file_path": audio_path
            }
            
            return parsed_result
            
        except Exception as e:
            print(f"å¤„ç†éŸ³é¢‘æ—¶å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                "error": f"å¤„ç†éŸ³é¢‘æ—¶å‡ºé”™: {str(e)}",
                "raw_response": "",
                "audio_info": {"file_path": audio_path}
            }

    def _parse_response(self, response: str) -> Dict:
        """
        è§£ææ¨¡å‹å“åº”ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯
        
        Args:
            response: æ¨¡å‹çš„åŸå§‹å“åº”æ–‡æœ¬
            
        Returns:
            è§£æåçš„ç»“æ„åŒ–æ•°æ®
        """
        result = {
            "overview": {},
            "speakers": [],
            "interaction": {},
            "raw_text": response
        }
        
        try:
            if "æ— æ³•ç†è§£éŸ³é¢‘å†…å®¹" in response:
                result["error"] = "æ¨¡å‹æ— æ³•ç†è§£éŸ³é¢‘å†…å®¹ï¼Œå¯èƒ½æ˜¯éŸ³é¢‘è´¨é‡é—®é¢˜æˆ–æ ¼å¼ä¸æ”¯æŒ"
                return result
            
            overview_match = re.search(r'ã€éŸ³é¢‘æ¦‚è§ˆã€‘(.*?)(?=ã€|$)', response, re.DOTALL)
            if overview_match:
                overview_text = overview_match.group(1)
                result["overview"] = self._extract_key_values(overview_text)
            
            # æå–æ¯ä¸ªè¯´è¯äººçš„ä¿¡æ¯
            speaker_pattern = r'\[è¯´è¯äºº(\d+)\](.*?)(?=\[è¯´è¯äºº|\ã€|$)'
            speaker_matches = re.finditer(speaker_pattern, response, re.DOTALL)
            
            for match in speaker_matches:
                speaker_id = match.group(1)
                speaker_text = match.group(2)
                
                speaker_info = {
                    "id": f"è¯´è¯äºº{speaker_id}",
                    "features": self._extract_speaker_features(speaker_text)
                }
                result["speakers"].append(speaker_info)
            
            interaction_match = re.search(r'ã€äº¤äº’ç‰¹å¾ã€‘(.*?)(?=ã€|$)', response, re.DOTALL)
            if interaction_match:
                interaction_text = interaction_match.group(1)
                result["interaction"] = self._extract_key_values(interaction_text)
            
        except Exception as e:
            print(f"è§£æå“åº”æ—¶å‡ºé”™: {e}")
            result["parse_error"] = str(e)
        
        return result
    
    def _extract_key_values(self, text: str) -> Dict:
        """æå–é”®å€¼å¯¹ä¿¡æ¯"""
        result = {}
        lines = text.strip().split('\n')
        for line in lines:
            if 'ï¼š' in line or ':' in line:
                parts = re.split('[ï¼š:]', line, 1)
                if len(parts) == 2:
                    key = parts[0].strip().replace('-', '').strip()
                    value = parts[1].strip()
                    if key and value:
                        result[key] = value
        return result
    
    def _extract_speaker_features(self, text: str) -> Dict:
        """æå–è¯´è¯äººç‰¹å¾"""
        features = {}
        patterns = {
            "å£°éŸ³ç‰¹å¾": r'å£°éŸ³ç‰¹å¾[ï¼š:](.*?)(?=\n[-â€¢]|\n[^\n]*[ï¼š:]|$)',
            "è¯­é€Ÿ": r'è¯­é€Ÿ[ï¼š:](.*?)(?=\n[-â€¢]|\n[^\n]*[ï¼š:]|$)',
            "è¯­è°ƒ": r'è¯­è°ƒ[ï¼š:](.*?)(?=\n[-â€¢]|\n[^\n]*[ï¼š:]|$)',
            "éŸ³é«˜": r'éŸ³é«˜[ï¼š:](.*?)(?=\n[-â€¢]|\n[^\n]*[ï¼š:]|$)',
            "æƒ…ç»ª": r'(?:ä¸»è¦)?æƒ…ç»ª[ï¼š:](.*?)(?=\n[-â€¢]|\n[^\n]*[ï¼š:]|$)',
            "å†…å®¹": r'è½¬å½•å†…å®¹[ï¼š:](.*?)(?=\n[-â€¢]|\n[^\n]*[ï¼š:]|$)'
        }
        
        for key, pattern in patterns.items():
            match = re.search(pattern, text, re.DOTALL)
            if match:
                features[key] = match.group(1).strip()
        
        return features
    
    def format_output(self, analysis_result: Dict) -> str:
        """
        æ ¼å¼åŒ–è¾“å‡ºåˆ†æç»“æœ
        
        Args:
            analysis_result: åˆ†æç»“æœå­—å…¸
            
        Returns:
            æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²è¾“å‡º
        """
        if "error" in analysis_result:
            return f"é”™è¯¯: {analysis_result['error']}"
        
        output = []
        output.append("="*60)
        output.append("éŸ³é¢‘åˆ†ææŠ¥å‘Š")
        output.append("="*60)
        
        # è¾“å‡ºéŸ³é¢‘ä¿¡æ¯
        if analysis_result.get("audio_info"):
            info = analysis_result["audio_info"]
            output.append(f"\nã€éŸ³é¢‘æ–‡ä»¶ä¿¡æ¯ã€‘")
            output.append(f"  æ–‡ä»¶è·¯å¾„: {info.get('file_path', 'N/A')}")
            output.append(f"  éŸ³é¢‘æ—¶é•¿: {info.get('duration', 0):.2f}ç§’")
            output.append(f"  é‡‡æ ·ç‡: {info.get('sample_rate', 0)}Hz")
        
        # è¾“å‡ºæ¦‚è§ˆ
        if analysis_result.get("overview"):
            output.append("\nã€éŸ³é¢‘æ¦‚è§ˆã€‘")
            for key, value in analysis_result["overview"].items():
                output.append(f"  {key}: {value}")
        
        # è¾“å‡ºè¯´è¯äººåˆ†æ
        if analysis_result.get("speakers"):
            output.append("\nã€è¯´è¯äººè¯¦ç»†åˆ†æã€‘")
            for speaker in analysis_result["speakers"]:
                output.append(f"\n{'-'*40}")
                output.append(f"ğŸ“¢ {speaker['id']}")
                output.append(f"{'-'*40}")
                
                features = speaker.get("features", {})
                for key, value in features.items():
                    if value:
                        output.append(f"  â–ª {key}: {value}")
        
        # è¾“å‡ºäº¤äº’åˆ†æ
        if analysis_result.get("interaction") and analysis_result["interaction"]:
            output.append("\nã€äº¤äº’ç‰¹å¾åˆ†æã€‘")
            for key, value in analysis_result["interaction"].items():
                output.append(f"  {key}: {value}")
        
        output.append("\n" + "="*60)
        
        return "\n".join(output)


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    AUDIO_PATH = "/root/autodl-fs/9.12.MP3"  # ä¿®æ”¹ä¸ºä½ çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    MODEL_DIR = None  # å¦‚æœå·²ä¸‹è½½æ¨¡å‹ï¼Œå¯ä»¥æŒ‡å®šè·¯å¾„ï¼›Noneåˆ™è‡ªåŠ¨ä¸‹è½½
    
    print("\n" + "="*60)
    print("Qwen2-Audio éŸ³é¢‘åˆ†æå·¥å…· - è¯´è¯äººåŒºåˆ†ç‰ˆ (ä¿®å¤ç‰ˆ)")
    print("="*60)
    
    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶
    if not os.path.exists(AUDIO_PATH):
        print(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {AUDIO_PATH}")
        print("è¯·ç¡®ä¿éŸ³é¢‘æ–‡ä»¶è·¯å¾„æ­£ç¡®")
        return
    
    # åˆå§‹åŒ–åˆ†æå™¨
    print("\nåˆå§‹åŒ–åˆ†æå™¨...")
    try:
        analyzer = AudioAnalyzer(model_dir=MODEL_DIR)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–åˆ†æå™¨å¤±è´¥: {e}")
        return
    
    # åˆ†æéŸ³é¢‘
    print(f"\nå¼€å§‹åˆ†æéŸ³é¢‘æ–‡ä»¶: {AUDIO_PATH}")
    print("æ­£åœ¨å¤„ç†ï¼Œè¯·ç¨å€™...")
    print("-" * 60)
    
    # æ‰§è¡Œåˆ†æ
    analysis_result = analyzer.analyze_full_audio(AUDIO_PATH)
    
    # æ ¼å¼åŒ–å¹¶è¾“å‡ºç»“æœ
    formatted_output = analyzer.format_output(analysis_result)
    print(formatted_output)
    
    # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºåŸå§‹å“åº”çš„å‰500ä¸ªå­—ç¬¦
    if "raw_response" in analysis_result and analysis_result["raw_response"]:
        print("\n" + "="*60)
        print("è°ƒè¯•ä¿¡æ¯ - åŸå§‹å“åº”é¢„è§ˆ:")
        print("="*60)
        print(analysis_result["raw_response"][:500] + "..." if len(analysis_result["raw_response"]) > 500 else analysis_result["raw_response"])
    
    # å¯é€‰ï¼šä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    save_to_file = input("\næ˜¯å¦ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶ï¼Ÿ(y/n): ")
    if save_to_file.lower() == 'y':
        output_file = AUDIO_PATH.rsplit('.', 1)[0] + "_analysis.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(formatted_output)
            if "raw_response" in analysis_result:
                f.write("\n\n" + "="*60)
                f.write("\nåŸå§‹å“åº”å†…å®¹ï¼š\n")
                f.write(analysis_result["raw_response"])
        print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # å¯é€‰ï¼šä¿å­˜JSONæ ¼å¼
    save_json = input("æ˜¯å¦ä¿å­˜JSONæ ¼å¼çš„ç»“æ„åŒ–æ•°æ®ï¼Ÿ(y/n): ")
    if save_json.lower() == 'y':
        json_file = AUDIO_PATH.rsplit('.', 1)[0] + "_analysis.json"
        # ç§»é™¤raw_responseä»¥å‡å°æ–‡ä»¶å¤§å°
        json_data = {k: v for k, v in analysis_result.items() if k != "raw_response"}
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSONæ•°æ®å·²ä¿å­˜åˆ°: {json_file}")


if __name__ == "__main__":
    main()
