# MCF: Text LLMS For Multimodal Emotional Causality

<div align="center">

[![arXiv](https://img.shields.io/badge/ğŸ“š%20Arxiv-Coming%20soon-ff0000)](#)
[![Dataset](https://img.shields.io/badge/ğŸ¤—%20Dataset-MCF-blueviolet)](https://modelscope.cn/datasets/zRzRzRzRzRzRzR/MCF)
</div>

## Data

Dataset task definition and annotation example of the MCF framework. The framework contains two core subtasks:
five-tuple
element extraction (identifying Target, Holder, Aspect, Opinion, Sentiment, and Rationale) and sentiment chain
analysis (constructing causal
relationship chains between emotional events).

![dataset.png](resources/dataset.png)

## MCF pipeline

The MCF (Multimodal Causality Framework) architecture. MCF employs a three-stage pipeline: Recognition extracts
multimodal features through adaptive fidelity control, Memory aggregates events to compress 200+ dialogue turns into 50-80 semantic
units, and Attribution performs cross-modal alignment and progressive reasoning to identify emotional causal chains. The framework
transforms multimodal dialogue sequences into structured representations processable by text-based LLMs while preserving long-distance causal
dependencies.

![structure.png](resources/structure.png)

# Result

Impact of MCF on LLMs Performance Across Different Evaluation Metrics. The scores are reported in percentage (%). â†“and +
indicate performance decrease and increase compared to GPT-o1(text-only), respectively. Bold values represent the best
performance in each
section. Causemotion uses only the text modal. LLM refers to using only the LLM itself.

![bench.png](resources/bench.png)

## å¿«é€Ÿå¼€å§‹

ç¡®ä¿è¿™äº›æ¨¡å‹å‡å·²ç»å®‰è£…ï¼š
`google-bert/bert-base-uncased`
`google/siglip-base-patch16-224`
`openai/whisper-large-v3`
`StarJiaxing/R1-Omni-0.5B`
`https://modelscope.cn/models/qwen/Qwen2-Audio-7B-Instruct`

æ¨¡å‹æ–‡ä»¶è·¯å¾„æ›¿æ¢ï¼š
ä¸Šè¿°æ¨¡å‹ä¸‹è½½å®Œåï¼Œä½ éœ€è¦åœ¨R1-Omni-0.5B/config.jsonä¸­æ›´æ”¹å¯¹åº”çš„æ¨¡å‹è·¯å¾„ï¼ˆç¬¬23ã€31è¡Œï¼‰ï¼š
```json
 "mm_audio_tower": "/path/to/local/models/whisper-large-v3",
 "mm_vision_tower": "/path/to/local/models/siglip-base-patch16-224"
```
å¹¶åˆ†åˆ«åœ¨vido.pyã€humanomni_arch.pyä¸­çš„ç¬¬122è¡Œä»¥åŠç¬¬83è¡Œæ›¿æ¢bertå®é™…è·¯å¾„ï¼š
```python
#vido.py
bert_model = ".cache/modelscope/hub/models/AI-ModelScope/bert-base-uncased" #æ›¿æ¢ä¸ºä½ çš„å®é™…è·¯å¾„
#humanomni_arch.py
bert_model = "/gpfs/work/aac/yulongli19/.cache/modelscope/hub/models/AI-ModelScope/bert-base-uncased" #æ›¿æ¢ä¸ºä½ çš„bertæ¨¡å‹è·¯å¾„
 ```

æ•°æ®é›†ï¼š
ä½ éœ€è¦å‡†å¤‡MP4è§†é¢‘æ–‡ä»¶ã€txtè½¬å½•æ–‡æœ¬æ–‡ä»¶ï¼Œä»¥åŠéŸ³é¢‘æ–‡ä»¶ï¼Œæ¨èä½¿ç”¨mp3æ ¼å¼ï¼š

|-video
    |-chat_1.mp4

|-chat
    |-chat_1.txt

|-audio
    |-chat_1.mp3


## è¿è¡Œ
1.å‡†å¤‡ç¯èŠ‚ï¼š
æˆ‘ä»¬æä¾›äº†convert_textè¿›è¡Œæ–‡æœ¬è½¬å½•:
```shell
cd convert_text
python main.py --input {chat.mp4} --api-key --parallel 8 --api-mode high
```
audio_convert.pyè¿›è¡ŒéŸ³é¢‘æå–ï¼š
```shell
cd main
python audio_convert.py --input_dir {mp4} --output_dir {mp3}
```
åœ¨ä¸‰ç§æ–‡ä»¶ï¼ˆmp3ï¼Œmp4ï¼Œtxtï¼‰éƒ½å‡†å¤‡å¥½åï¼Œé¦–å…ˆè¿›è¡ŒéŸ³é¢‘æå–ï¼š
```shell
cd main
python audio.py --audio_path --model_path --output_path
```
ç„¶åè¿›è¡Œè§†é¢‘æå–ï¼š
```shell
python video.py --root_dir {data} --output_dir --modal {audio or video_audio}
```
æ‹¼æ¥æå–åˆ°çš„å†…å®¹ï¼š
```shell
python combined.py --audio_dir {audio} --input_dir {video_info} --output_dir {output} 
```
æ‰€æœ‰çš„æå–åšå®Œä¹‹åï¼Œè¿›è¡Œå› æœé“¾çš„ç”Ÿæˆ
```shell
python get_emo_sw.py --input_dir --other_text {combined_text} --output_dir --config_path --llm_model --batch --window_sizes --step_sizes
```
è¯„åˆ†ä»£ç ä¸å˜ï¼ˆget_emo_scoreï¼‰ï¼š
```shell
python get_emo_score.py --gt_dir --input_dir --output_dir --batch --event_shreshold
```
